{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ISB Practical 3\n",
    "\n",
    "## Assignment Technical details\n",
    "Similar to Practical Report 2, please fill in the gaps in the code as indicated in the provided *Jupyter notebook* (folder 'Python computer practical' on ISB Blackboard) using Python 3 and, when finished, submit this notebook using ISB's Dropbox link before the submission deadline. Following each question in the notebook, you find \"cells\" for your answers, indicated either as <span style='color:Blue'>code answer</span> or <span style='color:Blue'>text answer</span>. **Only the changes you do to those cells will be evaluated, and please do not temper with the rest of the notebook, and only the provided Jupyter notebook will be considered for marking.**\n",
    "\n",
    "To complete the code for Practical Report 3, you need to understand the question (**Step 1**), the provided skeleton code (**Step 2**), and finally how to fill in the missing code (**Step 3**) using the variables etc from the remainder of the code. The skeleton mustn't be modified. Generally, the missing code should just be a few lines, and it is marked according to whether the report (code) runs (*Criterion 1*), whether it provides the correct result (*Criterion 2*), and whether the code is efficient (concise, elegant,...; *Criterion 3*). Regarding the latter criterion, this means that code is brief/minimal and based on the basic programming skills you learned in the first and second practical as well as the introductory Python lectures (to avoid cutting and pasting complicated or sophisticated code you may have found online). Make also sure figures have readable axis labels and visible line styles. Text answers need to be concise within word limits and do not require external references - they test if you understood and can interpret your results. Before submitting, make sure your notebook runs as a whole on *Anaconda* and produces the correct result! As some questions are harder, it is not expected that every student will be able to answer all subquestions. **Maximal total score for Practical 3 is 100 points.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring Turing patterns\n",
    "\n",
    "In 1952, Alan Turing published his famous reaction-diffusion model for two interacting molecular species. Counter- intuitively, he found that diffusion can lead to structure and pattern formation. Here, we explore how the initial homogeneous state becomes unstable using first a single cell, then two cells, and finally a 1-dimensional (1d) lattice of cells.\n",
    "\n",
    "![turing_rob-phillips.png](turing_rob-phillips.png)\n",
    "\n",
    "**FIG. 1:** Emergence of a Turing pattern for two interacting molecular species with diffusion in a pair of cells (A) and a 1d lattice of cells (B). Image thanks to Rob Phillips.\n",
    "\n",
    "Consider a single cell with two molecular species (or morphogens),  activator $X$ and inhibitor $Y$, \n",
    "described by the following set of dynamic equations\n",
    "\\begin{eqnarray}\n",
    "\\frac{dX}{dt}&=&5X - 6Y + 1\\\\\n",
    "\\frac{dY}{dt}&=&6X - 7Y + 1,\n",
    "\\end{eqnarray}\n",
    "where both $X$ and $Y$ are produced by $X$, along with some basal production, and degraded by $Y$. The steady state is $X=Y=1$, which is easily checked by setting $dX/dt=dY/dt=0$. \n",
    "\n",
    "**Question 1a:**  By introducing small deviations from the steady state via $X=1+x$ and $Y=1+y$, derive the Jacobian matrix. Define this matrix in your code below and calculate (and print) its eigenvalues $\\lambda_{1,2}$ through matrix diagonalisation with the *numpy* library. (Code answer, 5 points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Libraries\n",
    "import numpy as np\n",
    "\n",
    "# >>>>>> Students: answer Question 1a\n",
    "\n",
    "\n",
    "# <<<<<<<\n",
    "\n",
    "Feedback1a='PIII Q1a: '\n",
    "Mark1a=0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1b:** Explain whether the obtained steady state is stable or not. (Text answer - 20 words max, 5 points)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "Feedback1b='PIII Q1b: '\n",
    "Mark1b=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now consider two cells (<span style='color:Blue'>Fig. 1A</span>), where $X$ and $Y$ can exchange via diffusion:\n",
    "\\begin{eqnarray}\n",
    "\\frac{dX_1}{dt}&=&5X_1 - 6Y_1 + 1 + D_X(X_2 - X_1)\\\\\n",
    "\\frac{dY_1}{dt}&=&6X_1 - 7Y_1 + 1 + D_Y(Y_2 - Y_1)\\\\\n",
    "\\frac{dX_2}{dt}&=&5X_2 - 6Y_2 + 1 + D_X(X_1 - X_2)\\\\\n",
    "\\frac{dY_2}{dt}&=&6X_2 - 7Y_2 + 1 + D_Y(Y_1 - Y_2),\n",
    "\\end{eqnarray}\n",
    "where subscripts 1 and 2 refer to cells 1 and 2, and $D_X$ ($D_Y$) is the diffusion constant for species $X$ ($Y$) according to Fick's law.\n",
    "\n",
    "**Question 1c:** For a pair of cells with steady state $X_1=X_2=Y_1=Y_2=1$, explore the stability as a function of the diffusion constant $D_X$. For this purpose, set $D_X=0.5$ and plot largest eigenvalue for $D_Y$ from \n",
    "0.5 to 5. (Code answer, 10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# >>>>>> Students: answer Question 1c\n",
    "\n",
    "\n",
    "# <<<<<<\n",
    "\n",
    "Feedback1c='PIII Q1c: '\n",
    "Mark1c=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1d:** Explain result briefly (Text answer - 30 words max, 5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "Feedback1d='PIII Q1d: '\n",
    "Mark1d=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1e:** For $D_X=0.5$ and $D_Y=10$, watch the development of the pattern from an initial homogeneous state\n",
    "by plotting the solution of the four species as a function of time using *odeint* and the steady state without diffusion as initial state for time $t$ from 0 to 10. (Code answer, 10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Libraries\n",
    "import numpy as np\n",
    "from scipy.integrate import odeint\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Parameters\n",
    "T=10\n",
    "dT=0.01\n",
    "Dx=0.5\n",
    "Dy=2\n",
    "\n",
    "\n",
    "# >>>>>> Students: answer Question 1e\n",
    "\n",
    "\n",
    "\n",
    "# <<<<<<<<\n",
    "\n",
    "Feedback1e='PIII Q1e: '\n",
    "Mark1e=0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now consider a 1d lattice of cells (<span style='color:Blue'>Fig. 1B</span>) with linearised equations for cell $r$ given by\n",
    "\\begin{eqnarray}\n",
    "\\frac{dx_r}{dt}&=&A_1x_r + B_1y_r + D_X(x_{r+1} + x_{r-1} - 2x_r)\\\\\n",
    "\\frac{dy_r}{dt}&=&A_2x_r + B_2y_r + D_Y(y_{r+1} + y_{r-1} - 2y_r),\n",
    "\\end{eqnarray}\n",
    "where coefficients $A_1$, $B_1$, $A_2$, and $B_2$ are given by partial derivatives of nonlinear functions (Jacobian matrix). Plugging in trial solution $x_r(t)=x(t)e^{i2\\pi r/\\lambda}$ and $y_r(t)=y(t)e^{i2\\pi r/\\lambda}$ with wavelength of perturbation \n",
    "$\\lambda$ produces  \n",
    "\\begin{eqnarray}\n",
    "\\frac{dx}{dt}&=&\\left[A_1 + D_X\\left(e^{i2\\pi/\\lambda}+e^{-i2\\pi/\\lambda}-2\\right)\\right]x + B_1y\\\\\n",
    "\\frac{dy}{dt}&=&A_2x + \\left[B_2 + D_Y\\left(e^{i2\\pi/\\lambda}+e^{-i2\\pi/\\lambda}-2\\right)\\right]y.\n",
    "\\end{eqnarray}\n",
    "\n",
    "**Question 1f:** Assuming $\\lambda>\\!\\!>1$, use Taylor expansion $e^x\\approx 1+x+x^2/2$ to simplify above equations. Show that there is a wave-length specific instability by plotting largest eigenvalue (real part) as a function of $\\lambda$. Use $A_1=1$, $B_2=-1$, $A_2B_1=-1$, $D_X=1$ and $D_Y=100$. (Code answer, 10 points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#### Libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "# parameters\n",
    "pi = math.pi\n",
    "A1=1\n",
    "B2=-1\n",
    "Dx=1\n",
    "Dy=100\n",
    "A2=1\n",
    "B1=-1\n",
    "\n",
    "\n",
    "# >>>>>> Students: answer Question 1f\n",
    "\n",
    "\n",
    "\n",
    "# <<<<<<<<<\n",
    "\n",
    "\n",
    "Feedback1f='PIII Q1f: '\n",
    "Mark1f=0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1g:** Explain result. (Text answer - 30 words max, 5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "Feedback1g='PIII Q1g: '\n",
    "Mark1g=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The perceptron - learning patterns with a single neuron\n",
    "\n",
    "Neurons in the brain process input signals with different weights to produce an output, determining whether the neuron fires (1) or not (0) - see <span style='color:Blue'>Fig. 2</span>. Allowing a continuum of values, we can describe the output by a so-called activation function. This simplest of neural networks, called the *Perceptron*, was originally introduced by Frank Rosenblatt in 1957. A Perceptron can be turned into an algorithm for supervised learning of binary classifiers (where outputs are either 1 or 0). Here, we explore an algorithm which enables the perceptron to learn and process data points of a training data set one at a time, to learn the weights for matching inputs and correct outputs from the training data. In particular, for learning we will use the *generalised delta rule* to update the weights of the neuron. For more information, see e.g. https://en.wikipedia.org/wiki/Delta_rule or one of the many tutorials on the internet. However, all necessary information is provided here as well.\n",
    "\n",
    "\n",
    "![perceptron.png](perceptron.png)\n",
    "\n",
    "**FIG. 2:** Perceptron - a model for a single artificial neuron with input $x_i$ and weights $w_i$ with $i=1,2,3$, and output $y$. To learn the correct weights, the error, i.e. the mismatch between output and target, needs to be minimised with respect to the weights e.g. by using the method of stochastic gradient descent.\n",
    "\n",
    "The perceptron takes the inputs $x_1$, $x_2$, and $x_3$ and calculates the weighted sum as\n",
    "\\begin{eqnarray}\n",
    "v&=&wx + b\\\\\n",
    "&=&w_1 x_1+w_2x_2 + w_3x_3 + b\n",
    "\\end{eqnarray}\n",
    "with row vector $w=(w_1\\ w_2\\ w_3)$ and column vector $x = (x_1\\ x_2\\ x_3)^T$ with T the transpose, so that $wx$ is the dot product. Here we take the bias $b=0$. Finally, the neuron takes the weighted sum and converts it to the output $y$ using the activation function $g(x)$ (<span style='color:Blue'>Fig. 3</span>) via\n",
    "\\begin{equation}\n",
    "y=g(v).\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "![sigmoid.png](sigmoid.png)\n",
    "\n",
    "**FIG. 3:** Sigmoid activation function. The derivative is given by $g'(x)=g(x)\\cdot(1-g(x))$.\n",
    "\n",
    "The error between the target output and the actual output is given by\n",
    "\\begin{equation}\n",
    "e=t-y.\n",
    "\\end{equation}\n",
    "The training now loops through all the data points again and again (10,000 times or using 10,000 epochs) to update the weights until a threshold tolerance is reached, using the *generalised delta rule*:\n",
    "\\begin{equation}\n",
    "w_i\\longrightarrow w_i + \\Delta w_i\n",
    "\\end{equation}\n",
    "with $i=1, 2, 3$ and\n",
    "\\begin{eqnarray}\n",
    "\\Delta w_i&=& \\alpha\\cdot\\delta\\cdot x_i\\\\\n",
    "&=&\\alpha\\cdot g'(v)\\cdot e\\cdot x_i\n",
    "\\end{eqnarray}\n",
    "The minimisation of the error can be done with the *stochastic gradient descent* (SGD) algorithm.\n",
    "\n",
    "**Question 2a:** Take four training data points with inputs $(0, 0, 1), (0, 1, 1), (1, 0, 1), (1, 1, 1)$ and corresponding correct outputs $0, 0, 1, 1,$ and complete the following code by calculating step-by-step $v$, $y$, $e$, $\\delta$, and weight updates $\\delta w$ in function *DeltaSGD* and by completing the sigmoid activation function *Sigmoid*. Commands such as *np.dot* and *np.divide* or *np.multiply* might be useful (Code answer, 10 points)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-6-f45ae8b934a3>, line 30)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-6-f45ae8b934a3>\"\u001b[0;36m, line \u001b[0;32m30\u001b[0m\n\u001b[0;31m    X = np.array([[0, 0, 1], [0, 1, 1], [1, 0, 1], [1, 1, 1]])\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "import numpy as np  \n",
    "\n",
    "# >>>>>> Students: complete the gaps below as indicated\n",
    "\n",
    "def DeltaSGD(W, X, D):\n",
    "    ''' stochastic gradient descent using delta rule for learning'''\n",
    "    alpha=0.9\n",
    "    \n",
    "    N=4\n",
    "    for k in range(N):\n",
    "        x=X[k][:].transpose() # take each data point and convert row to column vector\n",
    "        d=D[k]\n",
    "        \n",
    "        # >>>>>\n",
    "        \n",
    "        \n",
    "        # <<<<<<\n",
    "        W=W+dW # new weights \n",
    "\n",
    "    return W\n",
    "\n",
    "def Sigmoid(x):\n",
    "    # >>>>>>>>>>>\n",
    "\n",
    "    # <<<<<<<<\n",
    "\n",
    "\n",
    "# train perceptron based on input X and output D\n",
    "\n",
    "X = np.array([[0, 0, 1], [0, 1, 1], [1, 0, 1], [1, 1, 1]])\n",
    "\n",
    "D = np.array([0, 0, 1, 1])  # data outputs\n",
    "\n",
    "W = 2*np.random.rand(1,3)-1   # initialise weight matrix\n",
    "\n",
    "for epoch in range(10000):  # 10000 rounds of learning\n",
    "    W = DeltaSGD(W, X, D)\n",
    "\n",
    "# print result \n",
    "print('Output:')\n",
    "N=4\n",
    "for k in range(N):\n",
    "    x=X[k][:]\n",
    "    v=np.dot(W,x)\n",
    "    print(Sigmoid(v)) # output\n",
    "    \n",
    "\n",
    "Feedback2a='PIII Q2a: '\n",
    "Mark2a=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2b:** Write new training perceptron code (second part of above code) now using new correct outputs $0, 1, 1, 0$, which is only a minor change to the code from before (Code answer, 5 points)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train again perceptron based on input X and new output D (XOR logic gate)\n",
    "\n",
    "# >>>>>> Students: do same for XOR\n",
    "\n",
    "\n",
    "# <<<<<<<\n",
    "\n",
    "\n",
    "Feedback2b='PIII Q2b: '\n",
    "Mark2b=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2c:** Explain your reults (Text answer - max. 50 words, 10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "Feedback2c='PIII Q2c: '\n",
    "Mark2c=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-means clustering for simplifying data\n",
    "\n",
    "Similar to dimensionality reduction, clustering algorithms can be immensely useful to simply complex data. One of the most common clustering algorithms in machine learning is known as *k-means* clustering. K-means clustering is a technique in which we place each observation in a dataset into one of $K$ clusters. The end goal is to have $K$ clusters in which the observations within each cluster are quite similar to each other while the observations in different clusters are quite different from each other. Here, we will explore this algorithm using the *KMeans* function from the *sklearn.cluster* module. Also provided to you are the data coordinates. See https://en.wikipedia.org/wiki/K-means_clustering or online tutorials for further information if needed.\n",
    "\n",
    "**Question 3a:** After reading the data file *cluster_data.txt*, make a scatter plot and fit four clusters to the data with *KMeans* and *fit* functions from the *sklearn.cluster* module. Also, print the coordinates of the cluster centres using *cluster_centers* feature. Note this problem requires you to research the K-means algoroithm and the *KMmeans* module. (Code answer, 10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400, 2)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load data and put in numpy array (make sure path is set correctly)\n",
    "content = np.loadtxt('cluster_data.txt')\n",
    "data=np.array(content)\n",
    "print(data.shape)\n",
    "\n",
    "# >>>>>> Students: complete Question 3a\n",
    "\n",
    "\n",
    "# <<<<<<<<\n",
    "\n",
    "Feedback3a='PIII Q3a: '\n",
    "Mark3a=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We often do not know beforehand how many clusters is optimal so we must create a plot that displays the number of clusters along with the sum of squared errors (SSE) of the model, i.e. the sum squared Euclidean distances between cluster centres and associated cluster data points. Typically when we create this type of plot we look for an “elbow” where the SSE begins to “bend” or level off. This is typically the optimal number of clusters.\n",
    "\n",
    "**Question 3b:** Complete the code below to find optimal number of clusters using an elbow plot based on SSE, exploring 1-10 clusters. Use again function *KMeans*, and its features such as *fit* and *inertia*. (Code answer, 10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# >>>>>> Students: complete Question 3b\n",
    "\n",
    "\n",
    "\n",
    "# <<<<<<\n",
    "\n",
    "Feedback3b='PIII Q3b: '\n",
    "Mark3b=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3c:** Interpret the \"elbow\" plot and explain your findings. (Text answer - max. 50 words, 5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "Feedback3c='PIII Q3c: '\n",
    "Mark3c=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PIII Q1a: \n",
      "0\n",
      "PIII Q1b: \n",
      "0\n",
      "PIII Q1c: \n",
      "0\n",
      "PIII Q1d: \n",
      "0\n",
      "PIII Q1e: \n",
      "0\n",
      "PIII Q1f: \n",
      "0\n",
      "PIII Q1g: \n",
      "0\n",
      "PIII Q2a: \n",
      "0\n",
      "PIII Q2b: \n",
      "0\n",
      "PII Q2c: \n",
      "0\n",
      "PIII Q3a: \n",
      "0\n",
      "PIII Q3c: \n",
      "0\n",
      "PIII Q3c: \n",
      "0\n",
      "total:  0\n"
     ]
    }
   ],
   "source": [
    "# print all feedback and marks\n",
    "\n",
    "print(Feedback1a)\n",
    "print(Mark1a)\n",
    "\n",
    "print(Feedback1b)\n",
    "print(Mark1b)\n",
    "\n",
    "print(Feedback1c)\n",
    "print(Mark1c)\n",
    "\n",
    "print(Feedback1d)\n",
    "print(Mark1d)\n",
    "\n",
    "print(Feedback1e)\n",
    "print(Mark1e)\n",
    "\n",
    "print(Feedback1f)\n",
    "print(Mark1f)\n",
    "\n",
    "print(Feedback1g)\n",
    "print(Mark1g)\n",
    "\n",
    "\n",
    "\n",
    "print(Feedback2a)\n",
    "print(Mark2a)\n",
    "\n",
    "print(Feedback2b)\n",
    "print(Mark2b)\n",
    "\n",
    "print(Feedback2c)\n",
    "print(Mark2c)\n",
    "\n",
    "\n",
    "\n",
    "print(Feedback3a)\n",
    "print(Mark3a)\n",
    "\n",
    "print(Feedback3b)\n",
    "print(Mark3b)\n",
    "\n",
    "print(Feedback3c)\n",
    "print(Mark3c)\n",
    "\n",
    "total_mark=Mark1a+Mark1b+Mark1c+Mark1d+Mark1e+Mark1f+Mark1g + Mark2a+Mark2b+Mark2c + Mark3a+Mark3b+Mark3c\n",
    "print('total: ',total_mark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
